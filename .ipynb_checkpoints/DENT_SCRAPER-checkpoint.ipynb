{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DENT SCRAPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupiter notebook is used to download, pre-process and save data about all dentists in the Czech Republic from the webpage of the Czech Chamber of Stomatology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  C:\\Users\\jhabetinek\\Desktop\\Škola\\Python\\Project_work\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting info about dentists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create class \"Dentist_Downloader\" that will help us to download data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dentist_Downloader:\n",
    "    \n",
    "    '''\n",
    "    Download manager class created for specific purpose: scraping informations about physician from https://www.dent.cz/zubni-lekari/ \n",
    "    \n",
    "    It contains methods for collection of links, downloading and storing data\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,allowLog=True):\n",
    "        \n",
    "    '''\n",
    "    Class creator\n",
    "    \n",
    "    Takes single argument : True or False, defining if you want to display status messages\n",
    "    \n",
    "    Creates class with attributes that are used to store data\n",
    "    '''\n",
    "        \n",
    "        self.allowLog = allowLog\n",
    "        self.links = {\n",
    "            'pages':[],\n",
    "            'dentists':[]\n",
    "        }\n",
    "        self.data = {\n",
    "            'name':[],\n",
    "            'wkp_name':[],\n",
    "            'street':[],\n",
    "            'pst_code':[],\n",
    "            'town':[],\n",
    "            'phone':[]\n",
    "        }\n",
    "\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Succesfully initialized Dentist Downloader')\n",
    "   \n",
    "    def getSubPages(self, link):\n",
    "    \n",
    "    '''\n",
    "    Method used to collect links to all sub-pages containing full list of dentists\n",
    "    \n",
    "    Takes single argument - starting URL\n",
    "    \n",
    "    Links to sub-pages stored in self.links['pages']\n",
    "    '''\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Searching for sub-pages on {} ...'.format(link))\n",
    "        \n",
    "        page = requests.get(link)\n",
    "        page.encoding = 'UTF-8'\n",
    "        soup = bs(page.text,'lxml')\n",
    "        \n",
    "        self.links['pages'] = ['https://www.dent.cz' + a['href'] for a in soup.findAll('a', attrs={'class' : 'btn'})][1:]\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Found {} sub-pages'.format(len(self.links['pages'])))\n",
    "            \n",
    "    def getDentists(self, link):\n",
    "    \n",
    "    '''\n",
    "    Method used to collect links to all individual dentists\n",
    "    \n",
    "    Takes single argument - list of links to sub-pages\n",
    "    \n",
    "    Links to individual dentists stored in self.links['dentists']\n",
    "    '''\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Searching for dentists on {} ...'.format(link))\n",
    "        \n",
    "        self.links['dentists'] = self.getLinks(link)\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Found {} dentists'.format(len(self.links['dentists'])))\n",
    "            \n",
    "    def getData(self, link):\n",
    "    \n",
    "    '''\n",
    "    Method used to collect data about dentists\n",
    "    \n",
    "    Takes single argument - list of links to individual dentists\n",
    "    \n",
    "    Data stored in self.data as a dictionary\n",
    "    '''\n",
    "    \n",
    "        if self.allowLog:\n",
    "            print('Searching for data about dentists on provided pages ...')\n",
    "        \n",
    "        self.data = self.scrapeChar(link)\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Successfully downloaded data about {} dentists'.format(len(self.data['name'])))\n",
    "            \n",
    "    def getLinks(self, links):\n",
    "        \n",
    "    '''\n",
    "    Method used to collect links to individual dentists applied in getDentists\n",
    "    \n",
    "    It is created in order to extract href atributes from BeautifulSoup objects stored in list of lists\n",
    "    \n",
    "    Returns single list with links to all dentists\n",
    "    '''\n",
    "        \n",
    "        HCP_links = []\n",
    "        \n",
    "        for page in tqdm(links):\n",
    "            content = requests.get(page)\n",
    "            content.encoding = 'UTF-8'\n",
    "            soup = bs(content.text,'lxml')\n",
    "\n",
    "            a_list = [ul.findAll('a') for ul in soup.findAll('ul', attrs={'class' : 'list-unstyled text-col-3'})]\n",
    "            a_merged = [val for sublist in a_list for val in sublist]\n",
    "            output = ['https://www.dent.cz' + a['href'] for a in a_merged]\n",
    "            HCP_links.append(output);\n",
    "            \n",
    "        HCP_links = [val for sublist in HCP_links for val in sublist]\n",
    "        \n",
    "        return HCP_links\n",
    "    \n",
    "    def scrapeChar(self, link, pause=0.1):\n",
    "    \n",
    "    '''\n",
    "    Method used to collect all data about dentists and their workplaces\n",
    "    \n",
    "    It is created in order to download all information at once\n",
    "    \n",
    "    Returns single dictionary\n",
    "    '''\n",
    "        \n",
    "        names = []\n",
    "        wkp_names = []\n",
    "        streets = []\n",
    "        pst_codes = []\n",
    "        towns = []\n",
    "        phones = []\n",
    "        \n",
    "        for page in tqdm(link):\n",
    "           \n",
    "            try:\n",
    "                content = requests.get(page)\n",
    "                content.encoding = 'UTF-8'\n",
    "                soup = bs(content.text,'lxml')\n",
    "\n",
    "                h_1 = soup.find('h1')\n",
    "                name = h_1.find('span', attrs={'itemprop' : 'name'}).text\n",
    "\n",
    "                subset = soup.findAll('address', attrs={'class' : 'text--condensed-vertical'})\n",
    "                wkp_name = self.parseSpan('name', subset)\n",
    "                street = self.parseSpan('streetAddress', subset)\n",
    "                pst_code = self.parseSpan('postalCode', subset)\n",
    "                town = self.parseSpan('addressLocality', subset)\n",
    "                phone = self.parseSpan('telephone', subset)\n",
    "    \n",
    "                names.append(name)\n",
    "                wkp_names.append(wkp_name)\n",
    "                streets.append(street)\n",
    "                pst_codes.append(pst_code)\n",
    "                towns.append(town)\n",
    "                phones.append(phone)\n",
    "            \n",
    "                time.sleep(pause)\n",
    "            \n",
    "            except: pass\n",
    "\n",
    "        stomatologists = {'name':names,\n",
    "                         'wkp_name':wkp_names,\n",
    "                         'street':streets,\n",
    "                         'pst_code':pst_codes,\n",
    "                         'town':towns,\n",
    "                         'phone':phones\n",
    "                         }\n",
    "        \n",
    "        return stomatologists\n",
    "    \n",
    "    def parseSpan(self, at, info):\n",
    "    \n",
    "    '''\n",
    "    Sub-method used to collect all data about dentists and their workplaces\n",
    "    \n",
    "    It is created in order to deal with the fact that not all information is always avaiable\n",
    "    \n",
    "    Returns single list of values\n",
    "    '''\n",
    "        chars = []\n",
    "        for item in info:\n",
    "            try:\n",
    "                char = item.find('span', attrs= {'itemprop' : at}).text\n",
    "                chars.append(char)\n",
    "            except:\n",
    "                char = 'NA'\n",
    "                chars.append(char)\n",
    "        \n",
    "        return chars\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our class, we will download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully initialized Dentist Downloader\n",
      "Searching for sub-pages on https://www.dent.cz/zubni-lekari/ ...\n",
      "Found 12 sub-pages\n",
      "Searching for dentists on ['https://www.dent.cz/zubni-lekari/A-B/', 'https://www.dent.cz/zubni-lekari/C-D/', 'https://www.dent.cz/zubni-lekari/E-F/', 'https://www.dent.cz/zubni-lekari/G-H/', 'https://www.dent.cz/zubni-lekari/I-J/', 'https://www.dent.cz/zubni-lekari/K-L/', 'https://www.dent.cz/zubni-lekari/M-N/', 'https://www.dent.cz/zubni-lekari/O-P/', 'https://www.dent.cz/zubni-lekari/Q-R/', 'https://www.dent.cz/zubni-lekari/S-T/', 'https://www.dent.cz/zubni-lekari/U-V/', 'https://www.dent.cz/zubni-lekari/W-Z/'] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:21<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10702 dentists\n",
      "Searching for data about dentists on provided pages ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10702/10702 [1:46:17<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data about 10699 dentists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "URL = 'https://www.dent.cz/zubni-lekari/'\n",
    "\n",
    "dentists = Dentist_Downloader()\n",
    "\n",
    "dentists.getSubPages(URL)\n",
    "\n",
    "dentists.getDentists(dentists.links['pages'])\n",
    "\n",
    "dentists.getData(dentists.links['dentists'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, the generally used methods and classes are sufficient - specificaly we use pandas to pre-process and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           MUDr. Amer Abed   \n",
      "1  DDS. Ahmad Amin Abosaleh   \n",
      "2  MUDr. Fatemeh Aboutorabi   \n",
      "3    MUDr. Marie Abrahamová   \n",
      "4     MUDr. Ahmad Abu Baker   \n",
      "\n",
      "                                            wkp_name  \\\n",
      "0  [MUDr. Amer Abed - Ortodoncie s.r.o., MUDr. Am...   \n",
      "1  [ARIES,  spol. s r.o. , keramické zubní středi...   \n",
      "2                                    [FAdent s.r.o.]   \n",
      "3                           [MUDr. Abrahamová Marie]   \n",
      "4                            [MUDr. Abu Baker Ahmad]   \n",
      "\n",
      "                                street          pst_code  \\\n",
      "0  [Masarykova 1132/62, Krokova 22/12]  [312 00, 360 01]   \n",
      "1                                   []          [552 03]   \n",
      "2             [Stroupežnického 522/18]          [150 00]   \n",
      "3                      [Francouzská 4]          [326 00]   \n",
      "4                 [Boleslavova 1136/4]          [460 06]   \n",
      "\n",
      "                    town                                  phone  \n",
      "0  [Plzeň, Karlovy Vary]  [377421542  , 353228572, 606163864  ]  \n",
      "1   [Velký Třebešov 118]                            [491453272]  \n",
      "2    [Praha 5 - Smíchov]   [257314738, 222522295 1, 774626262 ]  \n",
      "3                [Plzeň]                            [378014226]  \n",
      "4              [Liberec]                           [606367953 ]  \n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(dentists.data)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting rid of lists in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# examinig how many elements there are in one list at maximum\n",
    "print(data['wkp_name'].map(lambda x: len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 name                                           wkp_name  \\\n",
      "5301  MUDr. Jan Macko  [Stomatologie Macko s.r.o., MUDr. Macko Jan, S...   \n",
      "\n",
      "                                                 street  \\\n",
      "5301  [Soukalova 3355/3, Pštrossova 198/23, Pěnčín 3...   \n",
      "\n",
      "                                               pst_code  \\\n",
      "5301  [143 00, 110 00, 468 21, 155 00, 110 00, 468 4...   \n",
      "\n",
      "                                                   town  \\\n",
      "5301  [Praha 4, Praha 1, Bratříkov, Praha 5, Praha 1...   \n",
      "\n",
      "                                                  phone  \n",
      "5301  [241047230, NA, 483397693 , NA, NA, NA, NA, NA...  \n"
     ]
    }
   ],
   "source": [
    "# checking that such row indeed exists\n",
    "print(data[data['wkp_name'].map(lambda x: len(x)) == 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting individual elements in lists into separate columns\n",
    "\n",
    "colnames_wkp = ['wkp_name_' + str(i) for i in range(10)[1:10]]\n",
    "colnames_street = ['wkp_street_' + str(i) for i in range(10)[1:10]]\n",
    "colnames_psc = ['wkp_psc_' + str(i) for i in range(10)[1:10]]\n",
    "colnames_town = ['wkp_town_' + str(i) for i in range(10)[1:10]]\n",
    "colnames_phone = ['wkp_phone_' + str(i) for i in range(10)[1:10]]\n",
    "\n",
    "data[colnames_wkp] = pd.DataFrame(data['wkp_name'].values.tolist(), index= data.index)\n",
    "data[colnames_street] = pd.DataFrame(data['street'].values.tolist(), index= data.index)\n",
    "data[colnames_psc] = pd.DataFrame(data['pst_code'].values.tolist(), index= data.index)\n",
    "data[colnames_town] = pd.DataFrame(data['town'].values.tolist(), index= data.index)\n",
    "data[colnames_phone] = pd.DataFrame(data['phone'].values.tolist(), index= data.index)\n",
    "\n",
    "data = data.drop(['wkp_name','street','pst_code','town','phone'], axis = 1)\n",
    "data = data.replace(to_replace = 'NA', value = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('dentists_wide.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
