{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  C:\\Users\\jhabetinek\\Desktop\\Škola\\Python\\Project_work\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import os\n",
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting info about dentists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create class \"Dentist_Downloader\" that will help us to scrape effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dentist_Downloader:\n",
    "    \n",
    "    def __init__(self,allowLog=True):\n",
    "        \n",
    "        self.allowLog = allowLog\n",
    "        self.links = {\n",
    "            'pages':[],\n",
    "            'dentists':[]\n",
    "        }\n",
    "        self.data = []\n",
    "\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Succesfully initialized Dentist Downloader')\n",
    "   \n",
    "    def getSubPages(self, link):\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Searching for sub-pages on {} ...'.format(link))\n",
    "        \n",
    "        page = requests.get(link)\n",
    "        page.encoding = 'UTF-8'\n",
    "        soup = bs(page.text,'lxml')\n",
    "        \n",
    "        self.links['pages'] = ['https://www.dent.cz' + a['href'] for a in soup.findAll('a', attrs={'class' : 'btn'})][1:]\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Found {} sub-pages'.format(len(self.links['pages'])))\n",
    "            \n",
    "    def getDentists(self, link):\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Searching for dentists on {} ...'.format(link))\n",
    "        \n",
    "        self.links['dentists'] = self.getLinks(link)\n",
    "        \n",
    "        if self.allowLog:\n",
    "            print('Found {} dentists'.format(len(self.links['dentists'])))\n",
    "            \n",
    "    def getLinks(self, links):\n",
    "        \n",
    "        HCP_links = []\n",
    "        \n",
    "        for page in tqdm(links):\n",
    "            content = requests.get(page)\n",
    "            content.encoding = 'UTF-8'\n",
    "            soup = bs(content.text,'lxml')\n",
    "\n",
    "            a_list = [ul.findAll('a') for ul in soup.findAll('ul', attrs={'class' : 'list-unstyled text-col-3'})]\n",
    "            a_merged = [val for sublist in a_list for val in sublist]\n",
    "            output = ['https://www.dent.cz' + a['href'] for a in a_merged]\n",
    "            HCP_links.append(output);\n",
    "            \n",
    "        HCP_links = [val for sublist in HCP_links for val in sublist]\n",
    "        \n",
    "        return HCP_links\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully initialized Dentist Downloader\n",
      "Searching for sub-pages on https://www.dent.cz/zubni-lekari/ ...\n",
      "Found 12 sub-pages\n",
      "Searching for dentists on ['https://www.dent.cz/zubni-lekari/A-B/', 'https://www.dent.cz/zubni-lekari/C-D/', 'https://www.dent.cz/zubni-lekari/E-F/', 'https://www.dent.cz/zubni-lekari/G-H/', 'https://www.dent.cz/zubni-lekari/I-J/', 'https://www.dent.cz/zubni-lekari/K-L/', 'https://www.dent.cz/zubni-lekari/M-N/', 'https://www.dent.cz/zubni-lekari/O-P/', 'https://www.dent.cz/zubni-lekari/Q-R/', 'https://www.dent.cz/zubni-lekari/S-T/', 'https://www.dent.cz/zubni-lekari/U-V/', 'https://www.dent.cz/zubni-lekari/W-Z/'] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\n",
      "  8%|▊         | 1/12 [00:02<00:32,  3.00s/it]\n",
      " 17%|█▋        | 2/12 [00:06<00:32,  3.29s/it]\n",
      " 25%|██▌       | 3/12 [00:08<00:23,  2.64s/it]\n",
      " 33%|███▎      | 4/12 [00:10<00:20,  2.54s/it]\n",
      " 42%|████▏     | 5/12 [00:11<00:15,  2.15s/it]\n",
      " 50%|█████     | 6/12 [00:17<00:19,  3.19s/it]\n",
      " 58%|█████▊    | 7/12 [00:22<00:18,  3.72s/it]\n",
      " 67%|██████▋   | 8/12 [00:27<00:16,  4.06s/it]\n",
      " 75%|███████▌  | 9/12 [00:27<00:09,  3.05s/it]\n",
      " 83%|████████▎ | 10/12 [00:30<00:05,  2.83s/it]\n",
      " 92%|█████████▏| 11/12 [00:31<00:02,  2.33s/it]\n",
      "100%|██████████| 12/12 [00:32<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10702 dentists\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://www.dent.cz/zubni-lekari/'\n",
    "\n",
    "dentists = Dentist_Downloader()\n",
    "\n",
    "dentists.getSubPages(URL)\n",
    "dentists.getDentists(dentists.links['pages'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
